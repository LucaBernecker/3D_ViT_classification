{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddeec0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"torch\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "#from keras import ops\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302137a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1265, 32, 32, 32, 1)\n",
      "(1265,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.layers import Input, Conv3D, BatchNormalization, Activation, GlobalAveragePooling3D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models\n",
    "def load_data(file_path, dataset_name, start_idx=0, end_idx=None):\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        dataset = np.array(h5_file[dataset_name][start_idx:end_idx])\n",
    "    return dataset\n",
    "x_train_file = '/Users/lucabernecker/Desktop/N128_local/steno_32_xxl_seg/x_train.h5'\n",
    "y_train_file = '/Users/lucabernecker/Desktop/N128_local/steno_32_xxl_seg/y_train.h5'\n",
    "x_dataset_name = 'x_train'  # Adjust this based on the actual dataset name\n",
    "y_dataset_name = 'y_train'  # Adjust this based on the actual dataset name\n",
    "x_dataset_name2 = 'x_test'  # Adjust this based on the actual dataset name\n",
    "y_dataset_name2 = 'y_test'  # Adjust this based on the actual dataset name\n",
    "\n",
    "# Load data\n",
    "x_train = load_data(x_train_file, x_dataset_name)\n",
    "x_train = tf.expand_dims(x_train, axis=-1)\n",
    "y_train = load_data(y_train_file, y_dataset_name)\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(y_train))\n",
    "# Duplicate the binary labels to create two columns\n",
    "#y_train = np.stack((y_train, 1 - y_train), axis=1)\n",
    "x_test_file = '/Users/lucabernecker/Desktop/N128_local/steno_32_xxl_seg/x_test.h5'\n",
    "y_test_file = '/Users/lucabernecker/Desktop/N128_local/steno_32_xxl_seg/y_test.h5'\n",
    "\n",
    "# Load data\n",
    "x_test, y_test = load_data(x_test_file,x_dataset_name2), load_data(y_test_file,y_dataset_name2)\n",
    "x_test = tf.expand_dims(x_test, axis=-1)\n",
    "num_classes = 1\n",
    "input_shape = (32, 32, 32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0dc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 25\n",
    "num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n",
    "image_size = 100  # We'll resize input images to this size\n",
    "patch_size = 10  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 3\n",
    "projection_dim = 256\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024,\n",
    "]  # Size of the dense layers of the final classifier\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3016cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9fa237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = ops.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
    "        patches = ops.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f21367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c7481c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class RandomRotation3D(layers.Layer):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        if not training:\n",
    "            return inputs\n",
    "\n",
    "        # Generate random rotation angles for each axis\n",
    "        angles = self.factor * tf.random.uniform([3], -1, 1)\n",
    "        inputs = self.rotate_along_axis(inputs, angle=angles[0], axis=1)\n",
    "        inputs = self.rotate_along_axis(inputs, angle=angles[1], axis=2)\n",
    "        inputs = self.rotate_along_axis(inputs, angle=angles[2], axis=3)\n",
    "        return inputs\n",
    "\n",
    "    def rotate_along_axis(self, inputs, angle, axis):\n",
    "        # Create the rotation matrix\n",
    "        cos_angle = tf.cos(angle)\n",
    "        sin_angle = tf.sin(angle)\n",
    "        one = tf.ones_like(cos_angle)\n",
    "        zero = tf.zeros_like(cos_angle)\n",
    "\n",
    "        if axis == 1:\n",
    "            rotation_matrix = tf.convert_to_tensor([[one, zero, zero],\n",
    "                                                    [zero, cos_angle, -sin_angle],\n",
    "                                                    [zero, sin_angle, cos_angle]])\n",
    "        elif axis == 2:\n",
    "            rotation_matrix = tf.convert_to_tensor([[cos_angle, zero, sin_angle],\n",
    "                                                    [zero, one, zero],\n",
    "                                                    [-sin_angle, zero, cos_angle]])\n",
    "        else:\n",
    "            rotation_matrix = tf.convert_to_tensor([[cos_angle, -sin_angle, zero],\n",
    "                                                    [sin_angle, cos_angle, zero],\n",
    "                                                    [zero, zero, one]])\n",
    "\n",
    "        rotation_matrix = tf.reshape(rotation_matrix, [3, 3])\n",
    "        shape = tf.shape(inputs)\n",
    "        flat_inputs = tf.reshape(inputs, [-1, shape[1], shape[2], shape[3], shape[4]])\n",
    "        rotated_inputs = tf.einsum('bij,bjklm->biklm', rotation_matrix, flat_inputs)\n",
    "        return tf.reshape(rotated_inputs, shape)\n",
    "\n",
    "class RandomRotation3D(layers.Layer):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        if not training:\n",
    "            return inputs\n",
    "\n",
    "        # Generate random rotation angles for each axis\n",
    "        angles = self.factor * tf.random.uniform([3], -1, 1)\n",
    "        inputs = self.rotate_along_axis(inputs, angle=angles[0], axis=1)\n",
    "        inputs = self.rotate_along_axis(inputs, angle=angles[1], axis=2)\n",
    "        inputs = self.rotate_along_axis(inputs, angle=angles[2], axis=3)\n",
    "        return inputs\n",
    "\n",
    "    def rotate_along_axis(self, inputs, angle, axis):\n",
    "        # Create the rotation matrix\n",
    "        cos_angle = tf.cos(angle)\n",
    "        sin_angle = tf.sin(angle)\n",
    "        one = tf.ones_like(cos_angle)\n",
    "        zero = tf.zeros_like(cos_angle)\n",
    "\n",
    "        if axis == 1:\n",
    "            rotation_matrix = tf.convert_to_tensor([[one, zero, zero],\n",
    "                                                    [zero, cos_angle, -sin_angle],\n",
    "                                                    [zero, sin_angle, cos_angle]])\n",
    "        elif axis == 2:\n",
    "            rotation_matrix = tf.convert_to_tensor([[cos_angle, zero, sin_angle],\n",
    "                                                    [zero, one, zero],\n",
    "                                                    [-sin_angle, zero, cos_angle]])\n",
    "        else:\n",
    "            rotation_matrix = tf.convert_to_tensor([[cos_angle, -sin_angle, zero],\n",
    "                                                    [sin_angle, cos_angle, zero],\n",
    "                                                    [zero, zero, one]])\n",
    "\n",
    "        rotation_matrix = tf.reshape(rotation_matrix, [3, 3])\n",
    "        shape = tf.shape(inputs)\n",
    "        flat_inputs = tf.reshape(inputs, [-1, shape[2], shape[3], shape[4]])\n",
    "        rotated_inputs = tf.einsum('ij,bjklm->biklm', rotation_matrix, flat_inputs)\n",
    "        return tf.reshape(rotated_inputs, shape)\n",
    "class RandomZoom3D(layers.Layer):\n",
    "    def __init__(self, height_factor, width_factor, depth_factor):\n",
    "        super().__init__()\n",
    "        self.height_factor = height_factor\n",
    "        self.width_factor = width_factor\n",
    "        self.depth_factor = depth_factor\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        if not training:\n",
    "            return inputs\n",
    "\n",
    "        # Apply zoom on each dimension\n",
    "        zoom_factors = [1 + tf.random.uniform([], -self.depth_factor, self.depth_factor),\n",
    "                        1 + tf.random.uniform([], -self.height_factor, self.height_factor),\n",
    "                        1 + tf.random.uniform([], -self.width_factor, self.width_factor)]\n",
    "\n",
    "        shape = tf.shape(inputs)\n",
    "        zoomed = tf.image.resize(inputs, [tf.cast(shape[1] * zoom_factors[0], tf.int32),\n",
    "                                          tf.cast(shape[2] * zoom_factors[1], tf.int32),\n",
    "                                          tf.cast(shape[3] * zoom_factors[2], tf.int32)])\n",
    "\n",
    "        return tf.image.resize_with_crop_or_pad(zoomed, shape[1], shape[2], shape[3])\n",
    "\n",
    "# Assuming x_train has the shape (1265, 32, 32, 32, 1)\n",
    "print()\n",
    "# Assuming x_train has the shape (1265, 32, 32, 32)\n",
    "\n",
    "\n",
    "# Define the image size\n",
    "image_size = (32, 32, 32,1)\n",
    "\n",
    "# Define the data augmentation pipeline for 3D images\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "   #     layers.Resizing(image_size[0], image_size[1], image_size[2]),\n",
    "   #     layers.RandomFlip(\"horizontal\"),\n",
    "   #     RandomRotation3D(factor=0.0),\n",
    "   #     RandomZoom3D(height_factor=0, width_factor=0, depth_factor=0),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22f1b46e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/51 [..............................] - ETA: 7:31 - loss: 0.7572 - binary_accuracy: 0.7600"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 107\u001b[0m\n\u001b[1;32m    100\u001b[0m vit_classifier\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    101\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m    102\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Adjust loss function as needed\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mBinaryAccuracy()]  \u001b[38;5;66;03m# Adjust metrics as needed\u001b[39;00m\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m history \u001b[38;5;241m=\u001b[39m vit_classifier\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    108\u001b[0m     x_train, y_train,\n\u001b[1;32m    109\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test),\n\u001b[1;32m    110\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[1;32m    111\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    112\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m    113\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patches):\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        positions = tf.expand_dims(tf.range(start=0, limit=self.num_patches, delta=1), axis=0)\n",
    "        positions = tf.tile(positions, [batch_size, 1])\n",
    "        projected_patches = self.projection(patches)\n",
    "        encoded_patches = projected_patches[:, :self.num_patches, :] + self.position_embedding(positions)\n",
    "\n",
    "        # Slice to ensure consistent patch size\n",
    "        encoded_patches = encoded_patches\n",
    "\n",
    "        return encoded_patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config\n",
    "\n",
    "class Patches3D(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, volumes):\n",
    "        batch_size = tf.shape(volumes)[0]\n",
    "        depth = volumes.shape[1]\n",
    "        height = volumes.shape[2]\n",
    "        width = volumes.shape[3]\n",
    "        channels = volumes.shape[4]\n",
    "\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=tf.reshape(volumes, [-1, height, width, channels]),\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "\n",
    "        patch_depth = depth // self.patch_size\n",
    "        patch_height = height // self.patch_size\n",
    "        patch_width = width // self.patch_size\n",
    "\n",
    "        patches = tf.reshape(patches, [batch_size, patch_depth, patch_height, patch_width, -1])\n",
    "        patches = tf.transpose(patches, [0, 1, 2, 3, 4])\n",
    "        patches = tf.reshape(patches, [batch_size, -1, self.patch_size * self.patch_size * channels])\n",
    "\n",
    "  #      print(f\"Patches3D: volumes shape={volumes.shape}, patches shape={patches.shape}\")\n",
    "\n",
    "        return patches\n",
    "\n",
    "# Assuming x_train has the shape (1265, 32, 32, 32, 1)\n",
    "x_train_expanded = tf.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Now create the ViT classifier\n",
    "def create_vit_classifier(input_shape=(32, 32, 32, 1), num_classes=1):\n",
    "    patch_size = 4\n",
    "    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size) * (input_shape[2] // patch_size)\n",
    "    projection_dim = 256\n",
    "    transformer_layers = 8\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches3D(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "  #  print(f\"create_vit_classifier: input shape={input_shape}, patches shape={patches.shape}, encoded_patches shape={encoded_patches.shape}\")\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        encoded_patches = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=projection_dim, dropout=0.1)(encoded_patches, encoded_patches)\n",
    "        attention_output = layers.Dropout(0.1)(attention_output)\n",
    "        encoded_patches = layers.Add()([encoded_patches, attention_output])\n",
    "\n",
    "        feedforward_output = layers.Dense(projection_dim, activation=tf.nn.gelu)(encoded_patches)\n",
    "        feedforward_output = layers.Dropout(0.1)(feedforward_output)\n",
    "        encoded_patches = layers.Add()([encoded_patches, feedforward_output])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.1)(representation)\n",
    "    outputs = layers.Dense(units=num_classes, activation='sigmoid')(representation)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "vit_classifier = create_vit_classifier(input_shape=(32, 32, 32, 1))\n",
    "#vit_classifier.summary()\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "vit_classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='binary_crossentropy',  # Adjust loss function as needed\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]  # Adjust metrics as needed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = vit_classifier.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=25,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3a3c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1da4b3c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (10000, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m--> 118\u001b[0m vit_classifier \u001b[38;5;241m=\u001b[39m create_vit_classifier(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#vit_classifier.summary()\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "Cell \u001b[0;32mIn[147], line 113\u001b[0m, in \u001b[0;36mcreate_vit_classifier\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m    111\u001b[0m representation \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mFlatten()(representation)\n\u001b[1;32m    112\u001b[0m representation \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.1\u001b[39m)(representation)\n\u001b[0;32m--> 113\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(units\u001b[38;5;241m=\u001b[39mnum_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(representation)\n\u001b[1;32m    115\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:148\u001b[0m, in \u001b[0;36mDense.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    146\u001b[0m last_dim \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mdimension_value(input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe last dimension of the inputs to a Dense layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be defined. Found None. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull input shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_spec \u001b[38;5;241m=\u001b[39m InputSpec(min_ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m: last_dim})\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    156\u001b[0m     shape\u001b[38;5;241m=\u001b[39m[last_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    162\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (10000, None)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254bc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb528738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce2c3577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99af2095",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"masked_patch_encoder_1\" (type MaskedPatchEncoder).\n\nin user code:\n\n    File \"/var/folders/l5/n49gt_b5117516rctl7_nsx40000gn/T/ipykernel_53352/814724563.py\", line 16, in call  *\n        mask, mask_indices = mask_patches(patches, self.mask_ratio)\n    File \"/var/folders/l5/n49gt_b5117516rctl7_nsx40000gn/T/ipykernel_53352/612595237.py\", line 5, in mask_patches  *\n        num_masked = int(mask_ratio * num_patches)\n\n    TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n\n\nCall arguments received by layer \"masked_patch_encoder_1\" (type MaskedPatchEncoder):\n  • patches=tf.Tensor(shape=(None, None, 16), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vit_classifier \u001b[38;5;241m=\u001b[39m create_vit_classifier(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      2\u001b[0m vit_classifier\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      3\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m      4\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Use mean squared error for reconstruction\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m, in \u001b[0;36mcreate_vit_classifier\u001b[0;34m(input_shape, projection_dim, mask_ratio)\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m      7\u001b[0m patches \u001b[38;5;241m=\u001b[39m Patches3D(patch_size)(inputs)\n\u001b[0;32m----> 8\u001b[0m encoded_patches, mask, mask_indices \u001b[38;5;241m=\u001b[39m MaskedPatchEncoder(num_patches, projection_dim, mask_ratio)(patches)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(transformer_layers):\n\u001b[1;32m     11\u001b[0m     encoded_patches \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mLayerNormalization(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)(encoded_patches)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/l5/n49gt_b5117516rctl7_nsx40000gn/T/__autograph_generated_filedkbpmult.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, patches)\u001b[0m\n\u001b[1;32m     11\u001b[0m positions \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dims, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrange, (), \u001b[38;5;28mdict\u001b[39m(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, limit\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mnum_patches, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), fscope)\n\u001b[1;32m     12\u001b[0m positions \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtile, (ag__\u001b[38;5;241m.\u001b[39mld(positions), [ag__\u001b[38;5;241m.\u001b[39mld(batch_size), \u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 13\u001b[0m mask, mask_indices \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(mask_patches), (ag__\u001b[38;5;241m.\u001b[39mld(patches), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmask_ratio), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m visible_patches \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(patches) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(mask)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnewaxis])\n\u001b[1;32m     15\u001b[0m projected_patches \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mprojection, (ag__\u001b[38;5;241m.\u001b[39mld(visible_patches),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/l5/n49gt_b5117516rctl7_nsx40000gn/T/__autograph_generated_filexmc0cicz.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__mask_patches\u001b[0;34m(patches, mask_ratio)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m batch_size, num_patches, _ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(patches)\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 11\u001b[0m num_masked \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mint\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(mask_ratio) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(num_patches),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m mask_indices \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrange, (ag__\u001b[38;5;241m.\u001b[39mld(num_patches),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[:ag__\u001b[38;5;241m.\u001b[39mld(num_masked)]\n\u001b[1;32m     13\u001b[0m mask \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mscatter_nd, (), \u001b[38;5;28mdict\u001b[39m(indices\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dims, (ag__\u001b[38;5;241m.\u001b[39mld(mask_indices),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope), updates\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mones, (ag__\u001b[38;5;241m.\u001b[39mld(num_masked),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), shape\u001b[38;5;241m=\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(num_patches),)), fscope)\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"masked_patch_encoder_1\" (type MaskedPatchEncoder).\n\nin user code:\n\n    File \"/var/folders/l5/n49gt_b5117516rctl7_nsx40000gn/T/ipykernel_53352/814724563.py\", line 16, in call  *\n        mask, mask_indices = mask_patches(patches, self.mask_ratio)\n    File \"/var/folders/l5/n49gt_b5117516rctl7_nsx40000gn/T/ipykernel_53352/612595237.py\", line 5, in mask_patches  *\n        num_masked = int(mask_ratio * num_patches)\n\n    TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n\n\nCall arguments received by layer \"masked_patch_encoder_1\" (type MaskedPatchEncoder):\n  • patches=tf.Tensor(shape=(None, None, 16), dtype=float32)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995871f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patches):\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        positions = tf.expand_dims(tf.range(start=0, limit=self.num_patches, delta=1), axis=0)\n",
    "        positions = tf.tile(positions, [batch_size, 1])\n",
    "        projected_patches = self.projection(patches)\n",
    "        encoded_patches = projected_patches[:, :self.num_patches, :] + self.position_embedding(positions)\n",
    "\n",
    "        # Slice to ensure consistent patch size\n",
    "        encoded_patches = encoded_patches\n",
    "\n",
    "        return encoded_patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config\n",
    "\n",
    "class Patches3D(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, volumes):\n",
    "        batch_size = tf.shape(volumes)[0]\n",
    "        depth = volumes.shape[1]\n",
    "        height = volumes.shape[2]\n",
    "        width = volumes.shape[3]\n",
    "        channels = volumes.shape[4]\n",
    "\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=tf.reshape(volumes, [-1, height, width, channels]),\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "\n",
    "        patch_depth = depth // self.patch_size\n",
    "        patch_height = height // self.patch_size\n",
    "        patch_width = width // self.patch_size\n",
    "\n",
    "        patches = tf.reshape(patches, [batch_size, patch_depth, patch_height, patch_width, -1])\n",
    "        patches = tf.transpose(patches, [0, 1, 2, 3, 4])\n",
    "        patches = tf.reshape(patches, [batch_size, -1, self.patch_size * self.patch_size * channels])\n",
    "\n",
    "  #      print(f\"Patches3D: volumes shape={volumes.shape}, patches shape={patches.shape}\")\n",
    "\n",
    "        return patches\n",
    "\n",
    "# Assuming x_train has the shape (1265, 32, 32, 32, 1)\n",
    "x_train_expanded = tf.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Now create the ViT classifier\n",
    "def create_vit_classifier(input_shape=(32, 32, 32, 1), num_classes=1):\n",
    "    patch_size = 4\n",
    "    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size) * (input_shape[2] // patch_size)\n",
    "    projection_dim = 256\n",
    "    transformer_layers = 8\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches3D(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "  #  print(f\"create_vit_classifier: input shape={input_shape}, patches shape={patches.shape}, encoded_patches shape={encoded_patches.shape}\")\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        encoded_patches = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=projection_dim, dropout=0.1)(encoded_patches, encoded_patches)\n",
    "        attention_output = layers.Dropout(0.1)(attention_output)\n",
    "        encoded_patches = layers.Add()([encoded_patches, attention_output])\n",
    "\n",
    "        feedforward_output = layers.Dense(projection_dim, activation=tf.nn.gelu)(encoded_patches)\n",
    "        feedforward_output = layers.Dropout(0.1)(feedforward_output)\n",
    "        encoded_patches = layers.Add()([encoded_patches, feedforward_output])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.1)(representation)\n",
    "    outputs = layers.Dense(units=num_classes, activation='sigmoid')(representation)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "vit_classifier = create_vit_classifier(input_shape=(32, 32, 32, 1))\n",
    "#vit_classifier.summary()\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "vit_classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='binary_crossentropy',  # Adjust loss function as needed\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]  # Adjust metrics as needed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = vit_classifier.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=25,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
